{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Form Answer Type Freq Table.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"ke9KxOaZWdkx","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1647229777610,"user_tz":240,"elapsed":35952,"user":{"displayName":"Saptarashmi Bandyopadhyay","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16595046178311880344"}},"outputId":"a1cb7469-0326-43b6-f7f6-3e9e064ec78f"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/gdrive/\n"]}],"source":["#Step1. mount into drive\n","\n","from google.colab import drive\n","drive.mount(\"/content/gdrive/\")"]},{"cell_type":"code","source":["#Step 2. Go to the specific repository for creation of NQ-like data\n","\n","#Comment out the mkdir command below once the initial working repository is created\n","#!mkdir /content/gdrive/MyDrive/Quality-classifier/NQ-like-creation/\n","\n","%cd /content/gdrive/MyDrive/Quality-classifier/NQ-like-creation/"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uprcfFIQtzvP","executionInfo":{"status":"ok","timestamp":1647229943564,"user_tz":240,"elapsed":220,"user":{"displayName":"Saptarashmi Bandyopadhyay","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16595046178311880344"}},"outputId":"37080eae-ac2e-42c9-a64b-5380a718ab20"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/gdrive/MyDrive/Quality-classifier/NQ-like-creation\n"]}]},{"cell_type":"code","source":["#Step3. loading neuralcoref\n","#You may have to restart runtime after installing coref rerunning Step 2 \n","\n","#Comment out the git clone command below once neuralcoref is already downloaded\n","#!git clone https://github.com/huggingface/neuralcoref.git\n","\n","%cd neuralcoref\n","!pip install -r requirements.txt\n","!pip install -e ."],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"L4df_Esgbq30","executionInfo":{"status":"ok","timestamp":1647229879351,"user_tz":240,"elapsed":8211,"user":{"displayName":"Saptarashmi Bandyopadhyay","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16595046178311880344"}},"outputId":"f89f2626-dd20-4e56-af7b-3aaf5ddcd6f6"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/gdrive/MyDrive/Quality-classifier/NQ-like-creation/neuralcoref\n","Requirement already satisfied: spacy<3.0.0,>=2.1.0 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 1)) (2.2.4)\n","Requirement already satisfied: cython>=0.25 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 2)) (0.29.28)\n","Requirement already satisfied: pytest in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 3)) (3.6.4)\n","Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.0.0,>=2.1.0->-r requirements.txt (line 1)) (4.63.0)\n","Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.0.0,>=2.1.0->-r requirements.txt (line 1)) (0.4.1)\n","Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.0.0,>=2.1.0->-r requirements.txt (line 1)) (2.0.6)\n","Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.0.0,>=2.1.0->-r requirements.txt (line 1)) (3.0.6)\n","Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.0.0,>=2.1.0->-r requirements.txt (line 1)) (1.21.5)\n","Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.0.0,>=2.1.0->-r requirements.txt (line 1)) (0.9.0)\n","Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy<3.0.0,>=2.1.0->-r requirements.txt (line 1)) (1.0.0)\n","Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy<3.0.0,>=2.1.0->-r requirements.txt (line 1)) (1.1.3)\n","Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.0.0,>=2.1.0->-r requirements.txt (line 1)) (1.0.5)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy<3.0.0,>=2.1.0->-r requirements.txt (line 1)) (57.4.0)\n","Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.0.0,>=2.1.0->-r requirements.txt (line 1)) (7.4.0)\n","Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.0.0,>=2.1.0->-r requirements.txt (line 1)) (1.0.6)\n","Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.0.0,>=2.1.0->-r requirements.txt (line 1)) (2.23.0)\n","Requirement already satisfied: importlib-metadata>=0.20 in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy<3.0.0,>=2.1.0->-r requirements.txt (line 1)) (4.11.2)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy<3.0.0,>=2.1.0->-r requirements.txt (line 1)) (3.7.0)\n","Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy<3.0.0,>=2.1.0->-r requirements.txt (line 1)) (3.10.0.2)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.0.0,>=2.1.0->-r requirements.txt (line 1)) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.0.0,>=2.1.0->-r requirements.txt (line 1)) (2021.10.8)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.0.0,>=2.1.0->-r requirements.txt (line 1)) (2.10)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.0.0,>=2.1.0->-r requirements.txt (line 1)) (1.25.11)\n","Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.7/dist-packages (from pytest->-r requirements.txt (line 3)) (21.4.0)\n","Requirement already satisfied: more-itertools>=4.0.0 in /usr/local/lib/python3.7/dist-packages (from pytest->-r requirements.txt (line 3)) (8.12.0)\n","Requirement already satisfied: py>=1.5.0 in /usr/local/lib/python3.7/dist-packages (from pytest->-r requirements.txt (line 3)) (1.11.0)\n","Requirement already satisfied: atomicwrites>=1.0 in /usr/local/lib/python3.7/dist-packages (from pytest->-r requirements.txt (line 3)) (1.4.0)\n","Requirement already satisfied: pluggy<0.8,>=0.5 in /usr/local/lib/python3.7/dist-packages (from pytest->-r requirements.txt (line 3)) (0.7.1)\n","Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from pytest->-r requirements.txt (line 3)) (1.15.0)\n","Obtaining file:///content/gdrive/MyDrive/Quality-classifier/NQ-like-creation/neuralcoref\n","Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from neuralcoref==4.0) (1.21.5)\n","Requirement already satisfied: boto3 in /usr/local/lib/python3.7/dist-packages (from neuralcoref==4.0) (1.21.18)\n","Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from neuralcoref==4.0) (2.23.0)\n","Requirement already satisfied: spacy<3.0.0,>=2.1.0 in /usr/local/lib/python3.7/dist-packages (from neuralcoref==4.0) (2.2.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->neuralcoref==4.0) (2.10)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->neuralcoref==4.0) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->neuralcoref==4.0) (2021.10.8)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->neuralcoref==4.0) (1.25.11)\n","Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.0.0,>=2.1.0->neuralcoref==4.0) (4.63.0)\n","Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy<3.0.0,>=2.1.0->neuralcoref==4.0) (1.0.0)\n","Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.0.0,>=2.1.0->neuralcoref==4.0) (0.9.0)\n","Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy<3.0.0,>=2.1.0->neuralcoref==4.0) (1.1.3)\n","Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.0.0,>=2.1.0->neuralcoref==4.0) (0.4.1)\n","Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.0.0,>=2.1.0->neuralcoref==4.0) (2.0.6)\n","Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.0.0,>=2.1.0->neuralcoref==4.0) (1.0.5)\n","Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.0.0,>=2.1.0->neuralcoref==4.0) (3.0.6)\n","Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.0.0,>=2.1.0->neuralcoref==4.0) (1.0.6)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy<3.0.0,>=2.1.0->neuralcoref==4.0) (57.4.0)\n","Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.0.0,>=2.1.0->neuralcoref==4.0) (7.4.0)\n","Requirement already satisfied: importlib-metadata>=0.20 in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy<3.0.0,>=2.1.0->neuralcoref==4.0) (4.11.2)\n","Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy<3.0.0,>=2.1.0->neuralcoref==4.0) (3.10.0.2)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy<3.0.0,>=2.1.0->neuralcoref==4.0) (3.7.0)\n","Requirement already satisfied: botocore<1.25.0,>=1.24.18 in /usr/local/lib/python3.7/dist-packages (from boto3->neuralcoref==4.0) (1.24.18)\n","Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.7/dist-packages (from boto3->neuralcoref==4.0) (0.10.0)\n","Requirement already satisfied: s3transfer<0.6.0,>=0.5.0 in /usr/local/lib/python3.7/dist-packages (from boto3->neuralcoref==4.0) (0.5.2)\n","Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.7/dist-packages (from botocore<1.25.0,>=1.24.18->boto3->neuralcoref==4.0) (2.8.2)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.25.0,>=1.24.18->boto3->neuralcoref==4.0) (1.15.0)\n","Installing collected packages: neuralcoref\n","  Attempting uninstall: neuralcoref\n","    Found existing installation: neuralcoref 4.0\n","    Can't uninstall 'neuralcoref'. No files were found to uninstall.\n","  Running setup.py develop for neuralcoref\n","Successfully installed neuralcoref-4.0\n"]}]},{"cell_type":"code","source":["#Step3.5 Download necessary dataset\n","# Please comment out all git commands if the dataset is already downloaded\n","!sudo apt-get install git-lfs\n","!git lfs install\n","#Comment the data download command once git clone is done\n","#!git clone https://github.com/saptab/TriviaQuestion2NQ_Transform.git\n","%cd ./TriviaQuestion2NQ_Transform/TriviaQuestion2NQ_Transform_Dataset/"],"metadata":{"id":"WOGCZt_IgL8O"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Step4. import libraries\n","\n","import numpy as np\n","import pandas as pd\n","import json\n","import string\n","import nltk\n","import time\n","import os\n","import re\n","import random\n","import spacy\n","from collections import Counter\n","import neuralcoref\n","\n","nltk.download('punkt')\n","nltk.download('stopwords')\n","nltk.download('wordnet')\n","\n","nlp = spacy.load('en_core_web_sm')\n","neuralcoref.add_to_pipe(nlp)"],"metadata":{"id":"9Y_VilSKWjIl","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1647230005591,"user_tz":240,"elapsed":8711,"user":{"displayName":"Saptarashmi Bandyopadhyay","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16595046178311880344"}},"outputId":"972e3ac2-220c-4173-e3e5-b04ea6210b1a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 40155833/40155833 [00:02<00:00, 19274895.43B/s]\n"]},{"output_type":"stream","name":"stdout","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/wordnet.zip.\n"]},{"output_type":"execute_result","data":{"text/plain":["<spacy.lang.en.English at 0x7efecd855b90>"]},"metadata":{},"execution_count":6}]},{"cell_type":"code","source":["#Step5: functions for extraction the answer type for each sample\n","\n","remove_dict = {\n","    'For 10 points,  ':'', 'for 10 points,  ':'',\n","    'For ten points,  ':'', 'for ten points,  ':'',\n","    'FTP,  ':'', 'ftp,  ':'',\n","    'For 20 points,  ':'', 'for 20 points,  ':'',\n","    'For 5 points,  ':'',\n","    'For 10 points, ':'', 'for 10 points, ':'',\n","    'For ten points, ':'', 'for ten points, ':'',\n","    'FTP, ':'', 'ftp, ':'',\n","    'For 20 points, ':'', 'for 20 points, ':'',\n","    'For 5 points,':'', 'For 10 points — ':'',\n","    'For 10 points , ':'', 'for 10 points , ':'',\n","    'For ten points , ':'', 'for ten points , ':'',\n","    'FTP , ':'', 'ftp , ':'',\n","    'For 20 points , ':'', 'for 20 points , ':'',\n","    'For 5 points , ':'', \n","    'For 10 points ':'', 'for 10 points ':'',\n","    'For ten points ':'', 'for ten points ':'',\n","    'FTP ':'', 'ftp ':'',\n","    'For 20 points ':'', 'for 20 points ':'',\n","    'For 5 points ':''\n","}\n","\n","def uniques( your_string ):    \n","    words = your_string.split()\n","\n","    seen = set()\n","    seen_add = seen.add\n","\n","    def add(x):\n","        seen_add(x)  \n","        return x\n","    \n","    output = ' '.join( add(i) for i in words if i not in seen )\n","    return output\n","\n","def junk_last_sentence(q):\n","  # to make the last sentence start from the content after 'FTP's (name this/what)\n","  for k,v in remove_dict.items():\n","    index = q.find(k)\n","    if index!=-1:\n","      q = q[index:]\n","      break \n","  for k,v in remove_dict.items():\n","    q = re.sub(k, v, q)\n","  return q\n","\n","def get_answer_type(q):\n","  q = junk_last_sentence(q)\n","  word = \"\"\n","  # last sentence\n","  # find the answer type\n","  # simple case: extract NOUNs following 'name this's\n","  if q.split(' ')[:2] == ['name', 'this'] or q.split(' ')[:2] == ['identify', 'this'] or q.split(' ')[:2] == ['give', 'this'] or q.split(' ')[:2] == ['name', 'the'] \\\n","  or q.split(' ')[:2] == ['Name', 'this'] or q.split(' ')[:2] == ['Identify', 'this'] or q.split(' ')[:2] == ['Give', 'this'] or q.split(' ')[:2] == ['Name', 'the'] \\\n","  or q.split(' ')[:2] == ['Give', 'the'] or q.split(' ')[:2] == ['give', 'the']:\n","      doc = nlp(q)\n","      tok = []\n","      flag=0\n","      for i,token in enumerate(doc[2:12]):\n","        if token.pos_ == 'NOUN':\n","          #print('Noun Token = ', token)\n","          tok.append(str(doc[2:2+i]))\n","          tok.append(str(token))\n","          flag=1\n","        else:\n","          if flag:\n","            break\n","      word  = (' ').join(tok) # answer type\n","      # remove duplicates\n","      word = uniques(word)\n","      word  = word.strip()\n","      for k in ['\" ', ', ']:\n","        word = re.sub(k, '', word)\n","  elif q.split(' ')[:1] == ['what'] or q.split(' ')[:1] == ['What']:\n","      doc = nlp(q)\n","      tok = []\n","      flag=0\n","      for i,token in enumerate(doc[1:12]):\n","        if token.pos_ == 'NOUN':\n","          tok.append(str(doc[1:1+i]))\n","          tok.append(str(token))\n","          flag=1\n","        else:\n","          if flag:\n","            break\n","      word  = (' ').join(tok)\n","      for k in ['is this ', 'was this ', 'are these ', 'were these ']:\n","        word = re.sub(k, '', word)\n","      word = uniques(word)\n","      word  = word.strip()\n","      for k in ['\" ', ', ']:\n","        word = re.sub(k, '', word)\n","  elif q.split(' ')[:2] == ['name', 'these'] or q.split(' ')[:2] == ['identify', 'these'] or q.split(' ')[:2] == ['give', 'these'] \\\n","  or q.split(' ')[:2] == ['Name', 'these'] or q.split(' ')[:2] == ['Identify', 'these'] or q.split(' ')[:2] == ['Give', 'these']:\n","      doc = nlp(q)\n","      tok = []\n","      flag=0\n","      for i,token in enumerate(doc[2:12]):\n","        if token.pos_ == 'NOUN':\n","          #print('Noun Token = ', token)\n","          tok.append(str(doc[2:2+i]))\n","          tok.append(str(token))\n","          flag=1\n","        else:\n","          if flag:\n","            break\n","      word  = (' ').join(tok)\n","      word = uniques(word)\n","      word  = word.strip()\n","      for k in ['\" ', ', ']:\n","        word = re.sub(k, '', word)\n","  else:\n","      word = 'None'\n","  return word"],"metadata":{"id":"cR3I9J9kWjK3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Step6. get the answer type for each QB question\n","\n","def retrieve_answer_type_for_each_QB(orig_qb_path):\n","  if os.path.exists(orig_qb_path) == False:\n","    print('Please check if {} exists in the current folder'.format(orig_qb_path))\n","  f1 = open(orig_qb_path)\n","  qb_data = json.load(f1)['questions']\n","\n","  qanta_id = []\n","  qanta_questions_last = []\n","  qanta_questions_full = []\n","  qanta_answers = []\n","  qanta_page = []\n","  qanta_answer_type = []\n","  qanta_difficulty = []\n","  qanta_category = []\n","  qanta_subcategory = []\n","  qanta_year = []\n","\n","  for i in range(len(qb_data)): \n","    if i%5000 == 0:\n","      print(\"===> \"+str(i)+\"/112927\\n\")\n","    qanta_id.append(qb_data[i]['qanta_id'])\n","    qanta_questions_last.append(nltk.tokenize.sent_tokenize(qb_data[i]['text'])[-1])\n","    qanta_questions_full.append(qb_data[i]['text'])\n","    qanta_answers.append(qb_data[i]['answer'])\n","    qanta_page.append(qb_data[i]['page'])\n","    qanta_answer_type.append(get_answer_type(nltk.tokenize.sent_tokenize(qb_data[i]['text'])[-1]))\n","    qanta_difficulty.append(qb_data[i]['difficulty'])\n","    qanta_category.append(qb_data[i]['category'])\n","    qanta_subcategory.append(qb_data[i]['subcategory'])\n","    qanta_year.append(qb_data[i]['year'])\n","\n","  # save\n","  dataset1_lst = []\n","  for i in range(len(qb_data)):\n","      dataset1 = {}\n","      dataset1['qanta_id'] = qanta_id[i]\n","      dataset1['qanta_questions_last'] = qanta_questions_last[i]\n","      dataset1['qanta_questions_full'] = qanta_questions_full[i]\n","      dataset1['qanta_answers'] = qanta_answers[i]\n","      dataset1['qanta_page'] = qanta_page[i]\n","      dataset1['qanta_answer_type'] = qanta_answer_type[i]\n","      dataset1['qanta_difficulty'] = qanta_difficulty[i]\n","      dataset1['qanta_category'] = qanta_category[i]\n","      dataset1['qanta_subcategory'] = qanta_subcategory[i]\n","      dataset1['qanta_year'] = qanta_year[i]\n","      dataset1_lst.append(dataset1)\n","  with open(\"./qanta_train_with_answer_type.json\", 'w') as f:\n","      for item in dataset1_lst:\n","          f.write(json.dumps(item) + \"\\n\")\n","  return"],"metadata":{"id":"37xfb7GhI3vY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Step7. get the most freq answer type for each page\n","\n","def retrieve_most_freq_answer_type_for_qid(qanta_train_with_answer_type_path):\n","  if os.path.exists(qanta_train_with_answer_type_path) == False:\n","    print('Please check if {} exists in the current folder'.format(qanta_train_with_answer_type_path))\n","  qb_df = pd.read_json(qanta_train_with_answer_type_path, lines=True, orient='records')\n","\n","  page_to_answer_type_dict = {}\n","  for i in range(len(qb_df)):\n","    key = qb_df.iloc[i]['qanta_page']\n","    if key not in page_to_answer_type_dict:\n","        page_to_answer_type_dict[key] = list()\n","    page_to_answer_type_dict[key].append(qb_df.iloc[i]['qanta_answer_type'])\n","\n","  page_to_most_freq_answer_type_dict = {}\n","  for key in page_to_answer_type_dict.keys():\n","    lst = page_to_answer_type_dict[key]\n","    data = Counter(lst)\n","    most_freq = data.most_common(1)[0][0]\n","    page_to_most_freq_answer_type_dict[key] = most_freq\n","\n","  # qid to most freq answer type\n","  most_freq_answer_type_lst = []\n","  for i in range(len(qb_df)):\n","    most_freq_answer_type = page_to_most_freq_answer_type_dict[qb_df.iloc[i]['qanta_page']]\n","    most_freq_answer_type_lst.append(most_freq_answer_type)\n","\n","  qb_df['most_freq_answer_type'] = most_freq_answer_type_lst\n","\n","  qid_to_answer_type_dict = {}\n","  for i in range(len(qb_df)):\n","    qid_to_answer_type_dict[str(qb_df.iloc[i]['qanta_id'])] = qb_df.iloc[i]['most_freq_answer_type']\n","\n","  #save the most freq answer type for each qid into dictionary\n","  with open('./qanta_id_to_the_answer_type_most_freq_phrase_based_on_page_dict.json', 'w') as fp:\n","      json.dump(qid_to_answer_type_dict, fp)\n","  return"],"metadata":{"id":"lYU3ar_H_qt-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Step8. Main. Will take approximately 1.5 hours to obtain the entire dictionary over 112927 samples\n","#Dictionary is already provided in the data repo on github\n","\n","if __name__ == \"__main__\":\n","  orig_qb_path = './qanta.train.json'\n","  retrieve_answer_type_for_each_QB(orig_qb_path)\n","\n","  qanta_train_with_answer_type_path = './qanta_train_with_answer_type.json'\n","  retrieve_most_freq_answer_type_for_qid(qanta_train_with_answer_type_path)"],"metadata":{"id":"J9ciRvTQfAGC","colab":{"base_uri":"https://localhost:8080/"},"outputId":"a2d578e4-2f9c-470c-b5c6-b2ab048bef68"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["===> 0/112927\n","\n","===> 5000/112927\n","\n","===> 10000/112927\n","\n","===> 15000/112927\n","\n"]}]}]}