{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"QB_NQ_transformation_code_base_integrate.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"9098ae6e0a204953b11617eedf7b1219":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_2b0c6ff4ae784fc4b82492ac578c5351","IPY_MODEL_1bca9e056ab846c2a8b7566bbe8841b3"],"layout":"IPY_MODEL_34c79a1da8364f2d97c67aa47c449184"}},"2b0c6ff4ae784fc4b82492ac578c5351":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"Downloading: 100%","description_tooltip":null,"layout":"IPY_MODEL_ec1275eaba4f43c39bf45733fa4be128","max":28,"min":0,"orientation":"horizontal","style":"IPY_MODEL_aee24222a58b4003b7fefa8a3a60de02","value":28}},"1bca9e056ab846c2a8b7566bbe8841b3":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d759285535fc4b88baaac0639dbef427","placeholder":"​","style":"IPY_MODEL_fc52b623abee4a26a3de28d8dd9b20a2","value":" 28.0/28.0 [00:04&lt;00:00, 5.75B/s]"}},"34c79a1da8364f2d97c67aa47c449184":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ec1275eaba4f43c39bf45733fa4be128":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"aee24222a58b4003b7fefa8a3a60de02":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":"initial"}},"d759285535fc4b88baaac0639dbef427":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"fc52b623abee4a26a3de28d8dd9b20a2":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"2659925141304abea67ca31f354b2af9":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_62300d403bfc4a70893087094287accd","IPY_MODEL_6c82b5a5e4be4568b5e27df2781e4273"],"layout":"IPY_MODEL_1ff39d5d6b4b41dca1fe0e991f57ccaf"}},"62300d403bfc4a70893087094287accd":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"Downloading: 100%","description_tooltip":null,"layout":"IPY_MODEL_d99796f19a5b493683ff1ed4c88f0f8e","max":231508,"min":0,"orientation":"horizontal","style":"IPY_MODEL_b33214e7faf54959bd5ec9e8af6bca1b","value":231508}},"6c82b5a5e4be4568b5e27df2781e4273":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e2ec05f450a546ecb9770ed9820589ae","placeholder":"​","style":"IPY_MODEL_9d610d25ab5949d8a0074e46caaac512","value":" 226k/226k [00:00&lt;00:00, 645kB/s]"}},"1ff39d5d6b4b41dca1fe0e991f57ccaf":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d99796f19a5b493683ff1ed4c88f0f8e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b33214e7faf54959bd5ec9e8af6bca1b":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":"initial"}},"e2ec05f450a546ecb9770ed9820589ae":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9d610d25ab5949d8a0074e46caaac512":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f04ce59518384913b297844c91e7d888":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_eede79a6209344d494a9e427777bfa13","IPY_MODEL_508ec588b2694899a83e849bb672a464"],"layout":"IPY_MODEL_3b0035e12d004991a83a1f5a150ee041"}},"eede79a6209344d494a9e427777bfa13":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"Downloading: 100%","description_tooltip":null,"layout":"IPY_MODEL_16e2a23cc5b44b04aef18a39fac41555","max":466062,"min":0,"orientation":"horizontal","style":"IPY_MODEL_0c15793f068a47f4b363af81c489f9f4","value":466062}},"508ec588b2694899a83e849bb672a464":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_dc182d43133d46e982a3004e5ad9569f","placeholder":"​","style":"IPY_MODEL_ef64fe161997400dbed0f0e947a874ea","value":" 455k/455k [00:04&lt;00:00, 111kB/s]"}},"3b0035e12d004991a83a1f5a150ee041":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"16e2a23cc5b44b04aef18a39fac41555":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0c15793f068a47f4b363af81c489f9f4":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":"initial"}},"dc182d43133d46e982a3004e5ad9569f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ef64fe161997400dbed0f0e947a874ea":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ccdad00bcf7d4072b417ed7c9671b48b":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_7e4c0dde82ac4f9da88daad814679c33","IPY_MODEL_5f982f8698a747f593e2d04a96243639"],"layout":"IPY_MODEL_cbe7a081337a46cea858825f45b49fbd"}},"7e4c0dde82ac4f9da88daad814679c33":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"Downloading: 100%","description_tooltip":null,"layout":"IPY_MODEL_7c1b3ab747ca4b3b896584f2eb13e25d","max":483,"min":0,"orientation":"horizontal","style":"IPY_MODEL_6ce481d2f7514bc19e80cb6efff3c526","value":483}},"5f982f8698a747f593e2d04a96243639":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_33a11c3d35d84eada55077dab5d461df","placeholder":"​","style":"IPY_MODEL_f1298dfcc808487e98d98222ce602139","value":" 483/483 [00:00&lt;00:00, 880B/s]"}},"cbe7a081337a46cea858825f45b49fbd":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7c1b3ab747ca4b3b896584f2eb13e25d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6ce481d2f7514bc19e80cb6efff3c526":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":"initial"}},"33a11c3d35d84eada55077dab5d461df":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f1298dfcc808487e98d98222ce602139":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6j4YeM9iNfGy","outputId":"f6f5bbcc-a76e-4648-a4b5-77cd6002ef68","executionInfo":{"status":"ok","timestamp":1647236030447,"user_tz":240,"elapsed":888,"user":{"displayName":"Saptarashmi Bandyopadhyay","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16595046178311880344"}}},"source":["#Step 1. Mount the google drive\n","\n","from google.colab import drive\n","drive.mount(\"/content/gdrive/\")"],"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/gdrive/; to attempt to forcibly remount, call drive.mount(\"/content/gdrive/\", force_remount=True).\n"]}]},{"cell_type":"code","metadata":{"id":"Mw8qe_w7Sfgz","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1647233832319,"user_tz":240,"elapsed":132,"user":{"displayName":"Saptarashmi Bandyopadhyay","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16595046178311880344"}},"outputId":"2e65e86e-ee57-46bf-f9fa-cd2d537712fd"},"source":["#Step 2. Go to the specific repository for creation of NQ-like data\n","\n","#Comment out the mkdir command below once the initial working repository is created\n","#!mkdir /content/gdrive/MyDrive/Quality-classifier/NQ-like-creation/\n","\n","%cd /content/gdrive/MyDrive/Quality-classifier/NQ-like-creation/"],"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/gdrive/MyDrive/Quality-classifier/NQ-like-creation\n"]}]},{"cell_type":"code","source":["#Step3. Loading neuralcoref\n","#You may have to restart runtime after installing coref rerunning Step 2 \n","\n","%%shell\n","#Comment out the git clone command below once neuralcoref is already downloaded\n","#git clone https://github.com/huggingface/neuralcoref.git\n","\n","cd neuralcoref\n","pip install -r requirements.txt\n","pip install -e ."],"metadata":{"id":"qIlq7CJ9a4_D","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1647233681645,"user_tz":240,"elapsed":20566,"user":{"displayName":"Saptarashmi Bandyopadhyay","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16595046178311880344"}},"outputId":"d2361645-a5ab-4e21-8e6d-b2e0079e1063"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: spacy<3.0.0,>=2.1.0 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 1)) (2.2.4)\n","Requirement already satisfied: cython>=0.25 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 2)) (0.29.28)\n","Requirement already satisfied: pytest in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 3)) (3.6.4)\n","Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.0.0,>=2.1.0->-r requirements.txt (line 1)) (2.0.6)\n","Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.0.0,>=2.1.0->-r requirements.txt (line 1)) (3.0.6)\n","Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.0.0,>=2.1.0->-r requirements.txt (line 1)) (0.4.1)\n","Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.0.0,>=2.1.0->-r requirements.txt (line 1)) (0.9.0)\n","Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.0.0,>=2.1.0->-r requirements.txt (line 1)) (2.23.0)\n","Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.0.0,>=2.1.0->-r requirements.txt (line 1)) (1.0.6)\n","Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.0.0,>=2.1.0->-r requirements.txt (line 1)) (4.63.0)\n","Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy<3.0.0,>=2.1.0->-r requirements.txt (line 1)) (1.1.3)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy<3.0.0,>=2.1.0->-r requirements.txt (line 1)) (57.4.0)\n","Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.0.0,>=2.1.0->-r requirements.txt (line 1)) (1.0.5)\n","Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.0.0,>=2.1.0->-r requirements.txt (line 1)) (1.21.5)\n","Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy<3.0.0,>=2.1.0->-r requirements.txt (line 1)) (1.0.0)\n","Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.0.0,>=2.1.0->-r requirements.txt (line 1)) (7.4.0)\n","Requirement already satisfied: importlib-metadata>=0.20 in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy<3.0.0,>=2.1.0->-r requirements.txt (line 1)) (4.11.2)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy<3.0.0,>=2.1.0->-r requirements.txt (line 1)) (3.7.0)\n","Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy<3.0.0,>=2.1.0->-r requirements.txt (line 1)) (3.10.0.2)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.0.0,>=2.1.0->-r requirements.txt (line 1)) (1.24.3)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.0.0,>=2.1.0->-r requirements.txt (line 1)) (2.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.0.0,>=2.1.0->-r requirements.txt (line 1)) (2021.10.8)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.0.0,>=2.1.0->-r requirements.txt (line 1)) (3.0.4)\n","Requirement already satisfied: more-itertools>=4.0.0 in /usr/local/lib/python3.7/dist-packages (from pytest->-r requirements.txt (line 3)) (8.12.0)\n","Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from pytest->-r requirements.txt (line 3)) (1.15.0)\n","Requirement already satisfied: pluggy<0.8,>=0.5 in /usr/local/lib/python3.7/dist-packages (from pytest->-r requirements.txt (line 3)) (0.7.1)\n","Requirement already satisfied: py>=1.5.0 in /usr/local/lib/python3.7/dist-packages (from pytest->-r requirements.txt (line 3)) (1.11.0)\n","Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.7/dist-packages (from pytest->-r requirements.txt (line 3)) (21.4.0)\n","Requirement already satisfied: atomicwrites>=1.0 in /usr/local/lib/python3.7/dist-packages (from pytest->-r requirements.txt (line 3)) (1.4.0)\n","Obtaining file:///content/gdrive/MyDrive/Quality-classifier/NQ-like-creation/neuralcoref\n","Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from neuralcoref==4.0) (1.21.5)\n","Collecting boto3\n","  Downloading boto3-1.21.18-py3-none-any.whl (132 kB)\n","\u001b[K     |████████████████████████████████| 132 kB 3.2 MB/s \n","\u001b[?25hRequirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from neuralcoref==4.0) (2.23.0)\n","Requirement already satisfied: spacy<3.0.0,>=2.1.0 in /usr/local/lib/python3.7/dist-packages (from neuralcoref==4.0) (2.2.4)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->neuralcoref==4.0) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->neuralcoref==4.0) (2021.10.8)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->neuralcoref==4.0) (1.24.3)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->neuralcoref==4.0) (2.10)\n","Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.0.0,>=2.1.0->neuralcoref==4.0) (0.4.1)\n","Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy<3.0.0,>=2.1.0->neuralcoref==4.0) (1.0.0)\n","Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.0.0,>=2.1.0->neuralcoref==4.0) (4.63.0)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy<3.0.0,>=2.1.0->neuralcoref==4.0) (57.4.0)\n","Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.0.0,>=2.1.0->neuralcoref==4.0) (7.4.0)\n","Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.0.0,>=2.1.0->neuralcoref==4.0) (3.0.6)\n","Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.0.0,>=2.1.0->neuralcoref==4.0) (1.0.5)\n","Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy<3.0.0,>=2.1.0->neuralcoref==4.0) (1.1.3)\n","Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.0.0,>=2.1.0->neuralcoref==4.0) (1.0.6)\n","Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.0.0,>=2.1.0->neuralcoref==4.0) (0.9.0)\n","Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.0.0,>=2.1.0->neuralcoref==4.0) (2.0.6)\n","Requirement already satisfied: importlib-metadata>=0.20 in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy<3.0.0,>=2.1.0->neuralcoref==4.0) (4.11.2)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy<3.0.0,>=2.1.0->neuralcoref==4.0) (3.7.0)\n","Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy<3.0.0,>=2.1.0->neuralcoref==4.0) (3.10.0.2)\n","Collecting s3transfer<0.6.0,>=0.5.0\n","  Downloading s3transfer-0.5.2-py3-none-any.whl (79 kB)\n","\u001b[K     |████████████████████████████████| 79 kB 8.7 MB/s \n","\u001b[?25hCollecting jmespath<1.0.0,>=0.7.1\n","  Downloading jmespath-0.10.0-py2.py3-none-any.whl (24 kB)\n","Collecting botocore<1.25.0,>=1.24.18\n","  Downloading botocore-1.24.18-py3-none-any.whl (8.6 MB)\n","\u001b[K     |████████████████████████████████| 8.6 MB 23.4 MB/s \n","\u001b[?25hRequirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.7/dist-packages (from botocore<1.25.0,>=1.24.18->boto3->neuralcoref==4.0) (2.8.2)\n","Collecting urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1\n","  Downloading urllib3-1.25.11-py2.py3-none-any.whl (127 kB)\n","\u001b[K     |████████████████████████████████| 127 kB 50.4 MB/s \n","\u001b[?25hRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.25.0,>=1.24.18->boto3->neuralcoref==4.0) (1.15.0)\n","Installing collected packages: urllib3, jmespath, botocore, s3transfer, boto3, neuralcoref\n","  Attempting uninstall: urllib3\n","    Found existing installation: urllib3 1.24.3\n","    Uninstalling urllib3-1.24.3:\n","      Successfully uninstalled urllib3-1.24.3\n","  Running setup.py develop for neuralcoref\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\u001b[0m\n","Successfully installed boto3-1.21.18 botocore-1.24.18 jmespath-0.10.0 neuralcoref-4.0 s3transfer-0.5.2 urllib3-1.25.11\n"]},{"output_type":"execute_result","data":{"text/plain":[""]},"metadata":{},"execution_count":3}]},{"cell_type":"code","source":["#Step3.5 Download necessary dataset\n","# Please comment out all sudo and git commands if the dataset is already downloaded\n","#!sudo apt-get install git-lfs\n","#!git lfs install\n","#!git clone https://github.com/saptab/TriviaQuestion2NQ_Transform.git\n","%cd ./TriviaQuestion2NQ_Transform/TriviaQuestion2NQ_Transform_Dataset/"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bOUP001QhbGs","executionInfo":{"status":"ok","timestamp":1647233841627,"user_tz":240,"elapsed":134,"user":{"displayName":"Saptarashmi Bandyopadhyay","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16595046178311880344"}},"outputId":"3a24f7b7-a0f6-4123-fff3-dc1f636a539d"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/gdrive/MyDrive/Quality-classifier/NQ-like-creation/QA-MT-NLG/TriviaQuestion2NQ_Transform_Dataset\n"]}]},{"cell_type":"code","source":["#Step4. Installing corresponding libraries. \n","#You may have to redo Steps 2 and 3.5 for restarting runtime after installation\n","\n","!pip install -r \"requirements.txt\""],"metadata":{"id":"XDmMyE4GKuCW","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1647233853642,"user_tz":240,"elapsed":8954,"user":{"displayName":"Saptarashmi Bandyopadhyay","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16595046178311880344"}},"outputId":"a26f7c42-c104-497b-fdba-09eaf434cc19"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 1)) (4.17.0)\n","Requirement already satisfied: datasets in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 2)) (1.2.1)\n","Requirement already satisfied: faiss-cpu in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 3)) (1.7.2)\n","Requirement already satisfied: nltk>=3.6.0 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 4)) (3.7)\n","Requirement already satisfied: pytorch-lightning in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 5)) (1.5.10)\n","Requirement already satisfied: tqdm==4.43.0 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 6)) (4.43.0)\n","Requirement already satisfied: sacrebleu in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 7)) (2.0.0)\n","Requirement already satisfied: textstat in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 8)) (0.7.2)\n","Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 9)) (2.2.0)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from nltk>=3.6.0->-r requirements.txt (line 4)) (1.1.0)\n","Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.7/dist-packages (from nltk>=3.6.0->-r requirements.txt (line 4)) (2022.3.2)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from nltk>=3.6.0->-r requirements.txt (line 4)) (7.1.2)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers->-r requirements.txt (line 1)) (2.23.0)\n","Requirement already satisfied: tokenizers!=0.11.3,>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from transformers->-r requirements.txt (line 1)) (0.11.6)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers->-r requirements.txt (line 1)) (21.3)\n","Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers->-r requirements.txt (line 1)) (0.0.47)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers->-r requirements.txt (line 1)) (1.21.5)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers->-r requirements.txt (line 1)) (3.6.0)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers->-r requirements.txt (line 1)) (4.11.2)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers->-r requirements.txt (line 1)) (0.4.0)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers->-r requirements.txt (line 1)) (6.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers->-r requirements.txt (line 1)) (3.10.0.2)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers->-r requirements.txt (line 1)) (3.0.7)\n","Requirement already satisfied: xxhash in /usr/local/lib/python3.7/dist-packages (from datasets->-r requirements.txt (line 2)) (3.0.0)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from datasets->-r requirements.txt (line 2)) (1.3.5)\n","Requirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from datasets->-r requirements.txt (line 2)) (0.3.4)\n","Requirement already satisfied: pyarrow>=0.17.1 in /usr/local/lib/python3.7/dist-packages (from datasets->-r requirements.txt (line 2)) (6.0.1)\n","Requirement already satisfied: multiprocess in /usr/local/lib/python3.7/dist-packages (from datasets->-r requirements.txt (line 2)) (0.70.12.2)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers->-r requirements.txt (line 1)) (3.0.4)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers->-r requirements.txt (line 1)) (1.25.11)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers->-r requirements.txt (line 1)) (2.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers->-r requirements.txt (line 1)) (2021.10.8)\n","Requirement already satisfied: torch>=1.7.* in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning->-r requirements.txt (line 5)) (1.10.0+cu111)\n","Requirement already satisfied: fsspec[http]!=2021.06.0,>=2021.05.0 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning->-r requirements.txt (line 5)) (2022.2.0)\n","Requirement already satisfied: torchmetrics>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning->-r requirements.txt (line 5)) (0.7.2)\n","Requirement already satisfied: tensorboard>=2.2.0 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning->-r requirements.txt (line 5)) (2.8.0)\n","Requirement already satisfied: future>=0.17.1 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning->-r requirements.txt (line 5)) (0.18.2)\n","Requirement already satisfied: pyDeprecate==0.3.1 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning->-r requirements.txt (line 5)) (0.3.1)\n","Requirement already satisfied: setuptools==59.5.0 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning->-r requirements.txt (line 5)) (59.5.0)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.7/dist-packages (from fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning->-r requirements.txt (line 5)) (3.8.1)\n","Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning->-r requirements.txt (line 5)) (3.17.3)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning->-r requirements.txt (line 5)) (3.3.6)\n","Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning->-r requirements.txt (line 5)) (1.35.0)\n","Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning->-r requirements.txt (line 5)) (1.44.0)\n","Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning->-r requirements.txt (line 5)) (0.6.1)\n","Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning->-r requirements.txt (line 5)) (1.0.1)\n","Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning->-r requirements.txt (line 5)) (0.4.6)\n","Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning->-r requirements.txt (line 5)) (1.8.1)\n","Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning->-r requirements.txt (line 5)) (0.37.1)\n","Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning->-r requirements.txt (line 5)) (1.0.0)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from absl-py>=0.4->tensorboard>=2.2.0->pytorch-lightning->-r requirements.txt (line 5)) (1.15.0)\n","Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning->-r requirements.txt (line 5)) (4.2.4)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning->-r requirements.txt (line 5)) (4.8)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning->-r requirements.txt (line 5)) (0.2.8)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->pytorch-lightning->-r requirements.txt (line 5)) (1.3.1)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers->-r requirements.txt (line 1)) (3.7.0)\n","Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning->-r requirements.txt (line 5)) (0.4.8)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->pytorch-lightning->-r requirements.txt (line 5)) (3.2.0)\n","Requirement already satisfied: portalocker in /usr/local/lib/python3.7/dist-packages (from sacrebleu->-r requirements.txt (line 7)) (2.4.0)\n","Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.7/dist-packages (from sacrebleu->-r requirements.txt (line 7)) (0.8.9)\n","Requirement already satisfied: colorama in /usr/local/lib/python3.7/dist-packages (from sacrebleu->-r requirements.txt (line 7)) (0.4.4)\n","Requirement already satisfied: pyphen in /usr/local/lib/python3.7/dist-packages (from textstat->-r requirements.txt (line 8)) (0.12.0)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from sentence-transformers->-r requirements.txt (line 9)) (1.4.1)\n","Requirement already satisfied: sentencepiece in /usr/local/lib/python3.7/dist-packages (from sentence-transformers->-r requirements.txt (line 9)) (0.1.96)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from sentence-transformers->-r requirements.txt (line 9)) (1.0.2)\n","Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (from sentence-transformers->-r requirements.txt (line 9)) (0.11.1+cu111)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning->-r requirements.txt (line 5)) (21.4.0)\n","Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning->-r requirements.txt (line 5)) (2.0.12)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.7/dist-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning->-r requirements.txt (line 5)) (6.0.2)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning->-r requirements.txt (line 5)) (1.7.2)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning->-r requirements.txt (line 5)) (1.3.0)\n","Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.7/dist-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning->-r requirements.txt (line 5)) (4.0.2)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.7/dist-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning->-r requirements.txt (line 5)) (1.2.0)\n","Requirement already satisfied: asynctest==0.13.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning->-r requirements.txt (line 5)) (0.13.0)\n","Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets->-r requirements.txt (line 2)) (2018.9)\n","Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets->-r requirements.txt (line 2)) (2.8.2)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sentence-transformers->-r requirements.txt (line 9)) (3.1.0)\n","Requirement already satisfied: pillow!=8.3.0,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision->sentence-transformers->-r requirements.txt (line 9)) (7.1.2)\n"]}]},{"cell_type":"code","source":["#Step5. import libraries\n","\n","import numpy as np\n","import pandas as pd\n","import json\n","import string\n","import nltk\n","import time\n","import os\n","import re\n","import random\n","import argparse\n","import spacy\n","import neuralcoref\n","from nltk.tag import StanfordNERTagger\n","from nltk.tokenize import word_tokenize\n","import tensorflow as tf\n","from transformers import DistilBertTokenizerFast\n","from transformers import TFDistilBertForSequenceClassification\n","import faiss\n","from functools import partial\n","from sklearn.preprocessing import LabelEncoder\n","from sklearn.model_selection import train_test_split\n","import re\n","from collections import Counter\n","nltk.download('punkt')\n","nltk.download('stopwords')\n","nltk.download('wordnet')\n","nltk.download('averaged_perceptron_tagger')\n","import sys\n","sys.argv = ['']\n","\n","nlp = spacy.load('en_core_web_sm')\n","neuralcoref.add_to_pipe(nlp)\n","\n","#loading the answer type dictionary\n","with open('./qanta_id_to_the_answer_type_most_freq_phrase_based_on_page_dict.json') as json_file:\n","  answer_type_dict_before_parse_tree_nq_like_test_v_3 = json.load(json_file)\n","answer_type_dict = answer_type_dict_before_parse_tree_nq_like_test_v_3"],"metadata":{"id":"UoMpKCkXJ6wX","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1647233881844,"user_tz":240,"elapsed":25581,"user":{"displayName":"Saptarashmi Bandyopadhyay","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16595046178311880344"}},"outputId":"d9b17b01-e3e6-4fa9-de54-1ab77e31be26"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 40155833/40155833 [00:02<00:00, 18782377.89B/s]\n","[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/wordnet.zip.\n","[nltk_data] Downloading package averaged_perceptron_tagger to\n","[nltk_data]     /root/nltk_data...\n","[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"]}]},{"cell_type":"code","source":["#Step6. Updated Heuristics for NQlike quality checking\n","\n","# Heuristic 1 remove punctuation patterns at the beginning and the end of the question [\" ' ( ) , .]\n","def clean_marker(q):\n","  to_clean = \"\\\"|\\'|\\(|\\)|,|\\.\"\n","  has_heuristic = False\n","  q_array = q.split()\n","  array_leng = len(q_array)\n","  while re.match(to_clean, q_array[array_leng-1]):\n","    q_array = q_array[:array_leng-1]\n","    array_leng = array_leng - 1\n","    has_heuristic = True\n","\n","  while re.match(to_clean, q_array[0]):\n","    q_array = q_array[1:]\n","    array_leng = array_leng - 1\n","    has_heuristic = True\n","  if has_heuristic:\n","    q = ' '.join(q_array)\n","  return q\n","\n","# Heuristic 2 -- name this answer type correction\n","def clean_answer_type(q):\n","  to_clean = \"-- name this\"\n","  if re.search(to_clean, q):\n","    start_with = \"^-- name this\"\n","    # if start with -- name this converts to which\n","    if re.search(start_with, q):\n","      q = re.sub(start_with, 'which', q)\n","    else:\n","       q = re.sub(to_clean, 'the', q)\n","  return q\n","\n","# Heuristic 3 semicolon\n","def drop_after_semicolon(q):\n","  to_clean = \";.*\"\n","  if re.search(to_clean, q):\n","    q = re.sub(to_clean, '', q)\n","  return q \n","\n","# Heuristic 4 remove pattern issues\n","def remove_pattern(q):\n","  to_clean = \"Ã¢|Ã¢|â€‹|â€¦|â€•|â€˜ |â–º|Ã£|\\(\\s?\\*\\s?\\)|\\(\\s?\\+\\s?\\)|\\[\\s?\\*\\s?\\]|ftp,|for 10 points|for 10 points ,|for ten points ,|for 10points ,|ftp|--for 10 points--\"\n","  if re.search(to_clean, q):\n","    q = re.sub(to_clean, '', q)\n","  return q\n","\n","# Heuristic 5 remove repetition of the subject “is this” \n","def count_num_of_verbs(text, strictly = False):\n","  verb_tags = []\n","  if strictly:\n","    verb_tags = ['VB','VBD','VBN','VBP','VBZ']\n","  else:\n","    verb_tags = ['VB','VBD','VBG','VBN','VBP','VBZ']\n","  tokens = nltk.word_tokenize(text.lower())\n","  text = nltk.Text(tokens)\n","  tagged = nltk.pos_tag(text)\n","  counted = Counter(tag for word,tag in tagged)\n","  num_of_verb = 0\n","  for v in verb_tags:\n","    num_of_verb = num_of_verb + counted[v]\n","  return num_of_verb\n","\n","def remove_rep_subject(q):\n","  to_clean = \" is this [a-zA-Z]*\\s\"\n","  if re.search(to_clean, q):\n","    # the sentence has to have 1 verb at least otherwise this will not be done\n","    if (count_num_of_verbs(q) > 1):\n","      q = re.sub(to_clean, ' ', q)\n","  return q\n","\n","# Heuristic 6 change be determiner to s possession\n","def remove_bd(q):\n","  to_clean = \"( is his )|( is her )|( is its )\"\n","  if re.search(to_clean, q):\n","    q = re.sub(to_clean, '\\'s ', q)\n","  return q\n","\n","# Heuristic 7 add be verb to questions without verb\n","def add_verb(text):\n","  tokens = nltk.word_tokenize(text.lower())\n","  text = nltk.Text(tokens)\n","  tagged = nltk.pos_tag(text)\n","  ind = 0\n","  for tk,tg in tagged:\n","    if tg == 'NN' or tg == 'NNP':\n","      tokens.insert(ind+1,'is')\n","      break\n","    elif tg == 'NNS' or tg == 'NNPS':\n","      tokens.insert(ind+1,'are')\n","      break\n","    ind = ind + 1\n","  return ' '.join(tokens)\n","\n","def fix_no_verb(q):\n","  if (count_num_of_verbs(q, True) == 0):\n","    q = add_verb(q)\n","  return q\n","\n","# Heuristic 8 remove repetitive be verb when there's more verbs\n","def remove_rbv(q):\n","  to_clean = \"( is he )|( is she )|( is it )\"\n","  if re.search(to_clean, q):\n","    if (count_num_of_verbs(q) > 1):\n","      q = re.sub(to_clean, ' ', q)\n","  return q\n","\n","# Heuristic 9 First verb after which in continuous sense\n","def convert_fbawics(q):\n","  verb_tags = ['VB','VBD','VBG','VBN','VBP','VBZ']\n","  text = q\n","  tokens = nltk.word_tokenize(text.lower())\n","  text = nltk.Text(tokens)\n","  tagged = nltk.pos_tag(text)\n","  ind = 0\n","  for tk,tg in tagged:\n","    if tg in verb_tags:\n","      if tg == 'VBG':\n","        try:\n","          old_tk, old_tg = tagged[ind-1]\n","          if old_tg == 'NN' or old_tg == 'NNP':\n","            tokens[ind] = re.sub('ing','s',tokens[ind])\n","            q = ' '.join(tokens)\n","          else:\n","            tokens[ind] = re.sub('ing','',tokens[ind])\n","            q = ' '.join(tokens)\n","        except:\n","          break\n","        break\n","    else:\n","      break\n","    ind = ind + 1\n","  return q\n","\n","# Heuristic 10 fix \"name which\" \"identify which\"\n","def remove_niw(q):\n","  to_clean = \"identify which|name which\"\n","  if re.search(to_clean, q):\n","    q = re.sub(to_clean, 'which', q)\n","  return q\n","  \n","# function counts the number of of questions with 1,2,3 words\n","def count_word_freq(q_lst):\n","  count_1 = 0\n","  count_2 = 0\n","  count_3 = 0\n","  for q in q_lst:\n","    q_array = q.split()\n","    if len(q_array) == 1:\n","      count_1 = count_1 + 1\n","    if len(q_array) == 2:\n","      count_2 = count_2 + 1\n","    if len(q_array) == 3:\n","      count_3 = count_3 + 1\n","  return (count_1,count_2,count_3)\n","\n","# Heuristic11: convert 'this' to 'which' when no 'which' is present inside the question\n","def convert_this_to_which(q):\n","  x = q\n","  index = x.find('which')\n","  if index==-1:\n","    result = re.sub('this', 'which', x, 1)\n","    q = result\n","  return q\n","\n","# Heuristic12: replace 'this' to 'which'+answer_type within 'this is' pattern\n","def deal_with_this_is_pattern(qb_id, q):\n","  x = q\n","  index = x.find('this is')\n","  if index!=-1:\n","    # adding answer type\n","    qb_id = str(qb_id)\n","    if qb_id in answer_type_dict.keys():\n","      answer_type = answer_type_dict[qb_id] # get the answer type from qb_id\n","      replacement = 'which '+answer_type\n","      result = re.sub('this is', replacement+' is', x, 1)\n","      q = result\n","    else:\n","      # answer type is not in the dict\n","      result = re.sub('this', 'which', x, 1)\n","      q = result\n","  return q\n","\n","# Heuristic13: 'is/are' at the end of questions (after cleaning the wrong punc at the end of the sample)\n","def deal_with_end_be_verbs(q):\n","  x = q\n","  x = x.strip()\n","  if x[-3:] == ' is':\n","    result = x[:-3]\n","    q = result\n","  elif x[-4:] == ' are':\n","    result = x[:-4]\n","    q = result\n","  return q\n","\n","# Heuristic14: double be/AUX(pos) verbs\n","def deal_with_double_AUX(q):\n","  x = q\n","  doc_dep = nlp(x)\n","  lemma_lst = []\n","  tokem_text_lst = []\n","  for k in range(len(doc_dep)):\n","    lemma_lst.append(doc_dep[k].lemma_)\n","    tokem_text_lst.append(doc_dep[k].text)\n","  if lemma_lst.count('be') == 2:\n","    index = lemma_lst.index('be')\n","    if lemma_lst[index+1] == '-PRON-' and lemma_lst[index+2] == 'be':\n","      # two non-conjunctional be verbs with pronoun in between\n","      del tokem_text_lst[index+1]\n","      del tokem_text_lst[index+1]\n","      result = \" \".join(tokem_text_lst)\n","      q = result\n","    else:\n","      # two conjunction BE verbs or two non-conjunctional be verbs without pronoun in between\n","      del tokem_text_lst[index]\n","      result = \" \".join(tokem_text_lst)\n","      q = result\n","  return q\n","\n","# Heuristic15: 'which is where/why' pattern, convert 'which' to 'that' and check if no 'which' present anymore\n","# if so, convert 'this' to 'which'\n","def deal_with_WDT_BE_pattern(q):\n","  x = q\n","  index1 = x.find('which is where')\n","  index2 = x.find('which is why')\n","  if index1 != -1:\n","    result = re.sub('which is where', 'that is where', x)\n","    q = result\n","  elif index2 != -1:\n","    result = re.sub('which is why', 'that is why', x)\n","    q = result\n","  else:\n","    result = x\n","    # check if no 'which' present anymore\n","  index = result.find('which')\n","  if index==-1:\n","    result = re.sub('this', 'which', result, 1)\n","    q = result\n","  return q\n","\n","# Heuristic16: adding 'which+answer_type' at the beginning when no WDT/WRB present\n","# AFTER Heuristic1: 'which' checking\n","# WDT tag: which/what\n","# WRB tag: where/why/when\n","def deal_with_no_WDT(qb_id, q):\n","  x = q\n","  doc_dep = nlp(x)\n","  tag_lst = []\n","  tokem_text_lst = []\n","  for k in range(len(doc_dep)):\n","    tag_lst.append(doc_dep[k].tag_)\n","    tokem_text_lst.append(doc_dep[k].text)\n","  if ('WRB' in tag_lst)!=True and ('WDT' in tag_lst)!=True:\n","    # adding answer type at the beginning\n","    qb_id = str(qb_id)\n","    if qb_id in answer_type_dict.keys():\n","      answer_type = answer_type_dict[qb_id] # get the answer type from qb_id\n","      result = 'which '+answer_type+' is '+x\n","      q = result\n","    else:\n","      print(qb_id+'is not in the frequency table!')\n","  return q\n","\n","# Heuristic17: VERB/AUX (pos) at the beginning of the sample\n","def deal_with_VERB_AUX_at_beginning(qb_id, q):\n","  x = q\n","  doc_dep = nlp(x)\n","  pos_lst = []\n","  tokem_text_lst = []\n","  for k in range(len(doc_dep)):\n","    pos_lst.append(doc_dep[k].pos_)\n","    tokem_text_lst.append(doc_dep[k].text)\n","  if pos_lst[0]=='AUX' or pos_lst[0]=='VERB':\n","    # adding answer type at the beginning\n","    qb_id = str(qb_id)\n","    if qb_id in answer_type_dict.keys():\n","      answer_type = answer_type_dict[qb_id] # get the answer type from qb_id\n","      result = 'which '+answer_type+' '+x\n","      q = result\n","    else:\n","      print(qb_id+'is not in the frequency table!')\n","  return q\n","\n","# Heuristic18: convert 'which none is' to 'what is'\n","# AFTER Heuristic1: 'which' checking\n","def deal_which_none_is(qb_id, q):\n","  x = q\n","  index = x.find('which none is')\n","  if index != -1:\n","    qb_id = str(qb_id)\n","    if qb_id in answer_type_dict.keys():\n","      answer_type = answer_type_dict[qb_id] # get the answer type from qb_id\n","      result = re.sub('which none is', 'which '+answer_type+' is', x)\n","      q = result\n","    else:\n","      print(qb_id+'is not in the frequency table!')\n","  return q\n","\n","# Heuristic19: 'what is which' pattern\n","def deal_what_is_which(q):\n","  x = q\n","  index = x.find('what is which')\n","  if index != -1:\n","    result = re.sub('what is which', 'which', x)\n","    q = result\n","  return q\n","\n","# quality checking for each NQlike question\n","def quality_check(qb_id, q):\n","  remove_pattern(q)\n","  remove_niw(q)\n","  clean_marker(q)\n","  clean_answer_type(q)\n","  drop_after_semicolon(q)\n","  remove_rep_subject(q)\n","  remove_bd(q)\n","  remove_rbv(q)\n","  fix_no_verb(q)\n","  convert_fbawics(q)\n","  convert_this_to_which(q)\n","  deal_with_this_is_pattern(qb_id, q)\n","  deal_with_end_be_verbs(q)\n","  deal_with_double_AUX(q)\n","  deal_with_WDT_BE_pattern(q)\n","  deal_with_no_WDT(qb_id, q)\n","  deal_with_VERB_AUX_at_beginning(qb_id, q)\n","  deal_which_none_is(qb_id, q)\n","  deal_what_is_which(q)\n","  return "],"metadata":{"id":"omyfAM-Y-PJq","executionInfo":{"status":"ok","timestamp":1647233891488,"user_tz":240,"elapsed":1091,"user":{"displayName":"Saptarashmi Bandyopadhyay","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16595046178311880344"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["#Step7. pretrained BERT answer type classifier to indicate whether the answer type is A THING or A PERSON type\n","#No need to rerun the answer type classifier to replicate results as we are providing checkpoints for the same\n","#the checkpoints are already provided in the corresponding folder\n","\n","def answer_type_classifier_training():\n","    #No need to rerun the answer type classifier to replicate results as we are providing checkpoints for the same\n","    #the checkpoints are already provided in the corresponding folder\n","    \n","    #A PERSON: \n","    #     replace 'he/she/who/him' and 'He/She/Who/Him' with 'which + answer_type + is/are'\n","    #     replace 'his/whose/she's/he's' and 'His/Whose/She's/He's' with 'which + answer_type's'\n","    \n","    #A THING: \n","    #     replace 'it/this/these' and 'It/This/These' with 'which + answer_type + is/are'\n","    #     replace 'it's' and 'It's' with 'which + answer_type's'\n","    \n","    #manually annotated\n","    with open('./word_transform_dict.json', 'r') as f:\n","      last_sent_word_transform_30000 = json.load(f)\n","    \n","    person_list = []\n","    label_list = []\n","    for v in last_sent_word_transform_30000['who is the']:\n","      person_list.append(v)\n","      label_list.append('PERSON')\n","    \n","    non_person_list = []\n","    for v in last_sent_word_transform_30000['which is the']:\n","      non_person_list.append(v)\n","      label_list.append('NON-PERSON')\n","    \n","    for v in last_sent_word_transform_30000['what is the']:\n","      non_person_list.append(v)\n","      label_list.append('NON-PERSON')\n","    \n","    my_answer_type_list = person_list+non_person_list\n","    label_list = label_list[:len(my_answer_type_list)]\n","    # convert lists to dataframe\n","    zippedList =  list(zip(label_list, my_answer_type_list))\n","    classification_df = pd.DataFrame(zippedList, columns=['label','answer_type'])\n","    \n","    LE = LabelEncoder()\n","    classification_df['label'] = LE.fit_transform(classification_df['label'])\n","    classification_df.head()\n","    \n","    groups = classification_df['answer_type'].values.tolist()\n","    labels = classification_df['label'].tolist()\n","    \n","    training_sentences, validation_sentences, training_labels, validation_labels = train_test_split(groups, labels, test_size=.2)\n","    tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')\n","    tokenizer([training_sentences[0]], truncation=True,\n","                                padding=True, max_length=128)\n","    train_encodings = tokenizer(training_sentences,\n","                                truncation=True,\n","                                padding=True)\n","    val_encodings = tokenizer(validation_sentences,\n","                                truncation=True,\n","                                padding=True)\n","    train_dataset = tf.data.Dataset.from_tensor_slices((\n","        dict(train_encodings),\n","        training_labels\n","    ))\n","    \n","    val_dataset = tf.data.Dataset.from_tensor_slices((\n","        dict(val_encodings),\n","        validation_labels\n","    ))\n","    \n","    model = TFDistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased',num_labels=2)\n","    optimizer = tf.keras.optimizers.Adam(learning_rate=5e-5, epsilon=1e-08)\n","    model.compile(optimizer=optimizer, loss=model.compute_loss, metrics=['accuracy'])\n","    model.fit(train_dataset.shuffle(100).batch(16),\n","              epochs=3,\n","              batch_size=16,\n","              validation_data=val_dataset.shuffle(100).batch(16))\n","    \n","    #save the checkpoint\n","    model.save_pretrained(\"./BERT_Classification/Aug19_answer_type_classification_model/\")"],"metadata":{"id":"9AldDuHUKq0J"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Step8. initial heuristics for transforming QB to NQlike\n","\n","def clean_chunk(chunk):\n","  # might have trailing 'and', 'but', etc\n","  prefixes = ['and', 'but', 'when', 'while', ',']\n","  punc = ',.'\n","  chunk = chunk.strip()\n","  chunk = chunk.strip(punc)\n","  chunk = chunk.strip()\n","  chunk = chunk.strip(punc)\n","  chunk = chunk.strip()\n","  \n","  if chunk.endswith(' '):\n","    chunk = chunk[:-1]\n","  \n","  for prefix in prefixes:\n","    if chunk.startswith(prefix+' '):\n","      chunk =  chunk[len(prefix)+1:]\n","    if chunk.endswith(' '+prefix):\n","      chunk = chunk[:-len(prefix)-1]\n","  chunk = chunk.strip()\n","\n","  return chunk \n","\n","def uniques( your_string ):    \n","    words = your_string.split()\n","\n","    seen = set()\n","    seen_add = seen.add\n","\n","    def add(x):\n","        seen_add(x)  \n","        return x\n","    \n","    output = ' '.join( add(i) for i in words if i not in seen )\n","    return output\n","\n","def capitalization(q):\n","  # capitalize each sentences after parse tree/junk/answer_type extraction and before the transformation\n","  q = q[0].upper()+q[1:]\n","  return\n","\n","def remove_duplicates(q):\n","  words = q.split()\n","  for i, w in enumerate(words):\n","    if i >= (len(words)-1):\n","      continue\n","    w2 = words[i+1]\n","    w2 = re.sub('\\'s', '', w2)\n","    if w == w2:\n","      words = words[:i]+words[i+1:]\n","  q = \" \".join(words)\n","  return q\n","\n","# BERT answer type classification\n","tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')\n","loaded_model = TFDistilBertForSequenceClassification.from_pretrained(\"./BERT_Classification/Aug19_answer_type_classification_model/\")\n","def get_answer_type_group(test_sentence):\n","  predict_input = tokenizer.encode(test_sentence,\n","                                 truncation=True,\n","                                 padding=True,\n","                                 return_tensors=\"tf\")\n","  tf_output = loaded_model.predict(predict_input)[0]\n","  tf_prediction = tf.nn.softmax(tf_output, axis=1)\n","  labels = ['NON_PERSON','PERSON']\n","  label = tf.argmax(tf_prediction, axis=1)\n","  label = label.numpy()\n","  return labels[label[0]]\n","\n","def junk_last_sentence(q):\n","  # to make the last sentence start from the content after 'FTP's (name this/what) [Aug23: do not junk the content]\n","  # the content before 'FTP's merge it into previous sentence\n","  q_chunks = ''\n","  for k,v in remove_dict.items():\n","    index = q.find(k)\n","    if index!=-1:\n","      q_chunks = q[:index] # should merge to previous setence\n","      q = q[index:]\n","      break\n","  for k,v in remove_dict.items():\n","    q = re.sub(k, v, q)\n","  return q, q_chunks\n","\n","def last_sent_transform(q_with_the_chunks):\n","  q, q_chunks = junk_last_sentence(q_with_the_chunks)\n","  if q.split(' ')[:2] == ['name', 'this'] or q.split(' ')[:2] == ['identify', 'this'] or q.split(' ')[:2] == ['give', 'this'] or q.split(' ')[:2] == ['name', 'the'] \\\n","  or q.split(' ')[:2] == ['Name', 'this'] or q.split(' ')[:2] == ['Identify', 'this'] or q.split(' ')[:2] == ['Give', 'this'] or q.split(' ')[:2] == ['Name', 'the'] \\\n","  or q.split(' ')[:2] == ['Give', 'the'] or q.split(' ')[:2] == ['give', 'the']:\n","    doc = nlp(q)\n","    tok = []\n","    flag=0\n","    for i,token in enumerate(doc[2:6]):\n","      if token.pos_ == 'NOUN':\n","        #print('Noun Token = ', token)\n","        tok.append(str(token))\n","        flag=1\n","      else:\n","        if flag:\n","          break\n","    word  = (' ').join(tok)\n","    \n","    replacement = 'which is the'\n","    for k,v in last_sent_word_transform_30000.items():\n","      if k == 'unk':\n","        continue\n","      if word in v:\n","        replacement = k\n","        break\n","    \n","    transformed_q = q.split(' ')\n","    transformed_q = transformed_q[2:]\n","    transformed_q = (' ').join(transformed_q)\n","    transformed_q = replacement + ' ' + transformed_q   \n","  elif q.split(' ')[:2] == ['name', 'these'] or q.split(' ')[:2] == ['identify', 'these'] or q.split(' ')[:2] == ['give', 'these'] \\\n","  or q.split(' ')[:2] == ['Name', 'these'] or q.split(' ')[:2] == ['Identify', 'these'] or q.split(' ')[:2] == ['Give', 'these'] \\\n","  or q.split(' ')[:2] == ['Give', 'the'] or q.split(' ')[:2] == ['give', 'the']:\n","    doc = nlp(q)\n","    tok = []\n","    flag=0\n","    for i,token in enumerate(doc[2:6]):\n","      if token.pos_ == 'NOUN':\n","        #print('Noun Token = ', token)\n","        tok.append(str(token))\n","        flag=1\n","      else:\n","        if flag:\n","          break\n","    word  = (' ').join(tok)\n","    \n","    replacement = 'which are the'\n","    for k,v in last_sent_word_transform_30000.items():\n","      if not k == 'unk':\n","        continue\n","      if word in v:\n","        replacement = k\n","        break\n","    transformed_q = q.split(' ')\n","    transformed_q = transformed_q[2:]\n","    transformed_q = (' ').join(transformed_q)\n","    transformed_q = replacement + ' ' + transformed_q   \n","  else:\n","    transformed_q = q\n","  transformed_q = q_chunks+' '+transformed_q\n","  # remove adjancent duplicates\n","  q = remove_duplicates(q)\n","  q = q[0].lower()+q[1:]\n","  return transformed_q.strip()\n","\n","non_last_sent_transform_dict = {'this ':' which ', 'This ':'Which ',\n"," 'his ':'whose ', 'His ':'Whose ',\n","'these ':'which ', ''\n"," 'it ':' what ', 'its ': ' what\\'s ',\n"," 'It ':'What ', 'Its ':'What\\'s ',\n","    'After ':''\n"," }\n","\n","with open('./word_transform_dict.json', 'r') as f:\n","  last_sent_word_transform_30000 = json.load(f)\n","\n","remove_dict = {\n","    'For 10 points,  ':'', 'for 10 points,  ':'',\n","    'For ten points,  ':'', 'for ten points,  ':'',\n","    'FTP,  ':'', 'ftp,  ':'',\n","    'For 20 points,  ':'', 'for 20 points,  ':'',\n","    'For 5 points,  ':'',\n","    'For 10 points, ':'', 'for 10 points, ':'',\n","    'For ten points, ':'', 'for ten points, ':'',\n","    'FTP, ':'', 'ftp, ':'',\n","    'For 20 points, ':'', 'for 20 points, ':'',\n","    'For 5 points,':'', 'For 10 points — ':'',\n","    'For 10 points , ':'', 'for 10 points , ':'',\n","    'For ten points , ':'', 'for ten points , ':'',\n","    'FTP , ':'', 'ftp , ':'',\n","    'For 20 points , ':'', 'for 20 points , ':'',\n","    'For 5 points , ':'', \n","    'For 10 points ':'', 'for 10 points ':'',\n","    'For ten points ':'', 'for ten points ':'',\n","    'FTP ':'', 'ftp ':'',\n","    'For 20 points ':'', 'for 20 points ':'',\n","    'For 5 points ':''\n","}\n","\n","def transformation_intermediate_sent(qb_id, q):\n","  qb_id = str(qb_id)\n","  answer_type_dict = answer_type_dict_before_parse_tree_nq_like_test_v_3\n","  # capitalize the sentences after the answer_type extraction [Aug23: and deal with no pronous cases]\n","  capitalization(q)\n","\n","  qb_id = str(qb_id) # match the answer type from answer_type_dict\n","  q_orig = q\n","  FLAG = 0\n","  if qb_id in answer_type_dict.keys():\n","    answer_type = answer_type_dict[qb_id] # get the answer type from qb_id\n","    # detect if the answer_type (noun) is a person or a thing\n","    if answer_type in last_sent_word_transform_30000['who is the']:\n","      # answer_type is PERSON\n","      replacement_prefix = 'which'\n","      replacement = replacement_prefix+' '+answer_type\n","      # he/He/he's/He's/his/His/who/Who/whose/Whose\n","\n","      for k in ['He ', 'Who ', 'She ']:\n","        q = re.sub(k, replacement+' ', q, 1)\n","        if not q_orig == q:\n","          FLAG = 1\n","          break\n","      if FLAG:\n","        return steps_before_return(q)\n","      for k in ['This ']:\n","        q = re.sub(k, 'Which ', q, 1)\n","        if not q_orig == q:\n","          FLAG = 1\n","          break\n","      if FLAG:\n","        return steps_before_return(q)\n","      for k in [' he ', ' who ', ' she ', ' him ']:\n","        q = re.sub(k, ' '+replacement+' ', q, 1)\n","        if not q_orig == q:\n","          FLAG = 1\n","          break\n","      if FLAG:\n","        return steps_before_return(q)\n","      for k in [' this ']:\n","        q = re.sub(k, ' '+' which ', q, 1)\n","        if not q_orig == q:\n","          FLAG = 1\n","          break\n","      if FLAG:\n","        return steps_before_return(q)          \n","      for k in ['He\\'s ', 'His ', 'Whose ', 'She\\'s ', 'Her ']:\n","        q = re.sub(k, replacement+'\\'s'+' ', q, 1)   \n","        if not q_orig == q:\n","          FLAG = 1\n","          break\n","      if FLAG:\n","        return steps_before_return(q)     \n","      for k in [' he\\'s ', ' his ', ' whose ', ' she\\'s ', ' her ']:\n","        q = re.sub(k, ' '+replacement+'\\'s'+' ', q, 1)\n","        if not q_orig == q:\n","          FLAG = 1\n","          break\n","      if FLAG:\n","        return steps_before_return(q)\n","    # answer type is not in the last_sent_word_transform_30000 dictionary\n","    else:\n","      # classified as PERSON by BERT\n","      classification_output = get_answer_type_group(answer_type)\n","      if classification_output == 'PERSON':\n","        # answer_type is PERSON\n","        replacement_prefix = 'which'\n","        replacement = replacement_prefix+' '+answer_type\n","        # he/He/he's/He's/his/His/who/Who/whose/Whose\n","        for k in ['He ', 'Who ', 'She ']:\n","          q = re.sub(k, replacement+' ', q, 1)\n","          if not q_orig == q:\n","            FLAG = 1\n","            break\n","        if FLAG:\n","          return steps_before_return(q) \n","        for k in ['This ']:\n","          q = re.sub(k, 'Which ', q, 1)      \n","          if not q_orig == q:\n","            FLAG = 1\n","            break\n","        if FLAG:\n","          return steps_before_return(q) \n","        for k in [' he ', ' who ', ' she ', ' him ']:\n","          q = re.sub(k, ' '+replacement+' ', q, 1)\n","          if not q_orig == q:\n","            FLAG = 1\n","            break\n","        if FLAG:\n","          return steps_before_return(q) \n","        for k in [' this ']:\n","          q = re.sub(k, ' which ', q, 1)\n","          if not q_orig == q:\n","            FLAG = 1\n","            break\n","        if FLAG:\n","          return steps_before_return(q) \n","        for k in ['He\\'s ', 'His ', 'Whose ', 'She\\'s ', 'Her ']:\n","          q = re.sub(k, replacement+'\\'s'+' ', q, 1)\n","          if not q_orig == q:\n","            FLAG = 1\n","            break\n","        if FLAG:\n","          return steps_before_return(q) \n","        for k in [' he\\'s ', ' his ', ' whose ', ' she\\'s ', ' her ']:\n","          q = re.sub(k, ' '+replacement+'\\'s'+' ', q, 1)\n","          if not q_orig == q:\n","            FLAG = 1\n","            break\n","        if FLAG:\n","          return steps_before_return(q) \n","      else:\n","        # answer_type is a thing \n","        replacement_prefix = 'which'\n","        replacement = replacement_prefix+' '+answer_type\n","        # swap in with the replacement\n","        # what/What/what's/What's/it/It/it's/It's/its/Its -> what/What+replacement\n","        for k in ['What ', 'It ']:\n","          q = re.sub(k, replacement+' ', q, 1)\n","          if not q_orig == q:\n","            FLAG = 1\n","            break\n","        if FLAG:\n","          return steps_before_return(q) \n","        for k in ['This ']:\n","          q = re.sub(k, 'Which ', q, 1)\n","          if not q_orig == q:\n","            FLAG = 1\n","            break\n","        if FLAG:\n","          return steps_before_return(q) \n","        for k in [' what ', ' it ']:\n","          q = re.sub(k, ' '+replacement+' ', q, 1)\n","          if not q_orig == q:\n","            FLAG = 1\n","            break\n","        if FLAG:\n","          return steps_before_return(q) \n","        for k in [' this ']:\n","          q = re.sub(k, ' which ', q, 1)         \n","          if not q_orig == q:\n","            FLAG = 1\n","            break\n","        if FLAG:\n","          return steps_before_return(q) \n","        for k in ['What\\'s ', 'Its ', 'It\\'s ']:\n","          q = re.sub(k, replacement+'\\'s'+' ', q, 1)\n","          if not q_orig == q:\n","            FLAG = 1\n","            break\n","        if FLAG:\n","          return steps_before_return(q) \n","        for k in [' what\\'s ', ' its ', ' it\\'s ']:\n","          q = re.sub(k, ' '+replacement+'\\'s'+' ', q, 1)\n","          if not q_orig == q:\n","            FLAG = 1\n","            break\n","        if FLAG:\n","          return steps_before_return(q) \n","  else:\n","      for k,v in non_last_sent_transform_dict.items():\n","        q = re.sub(' '+k, ' '+v, q, 1)\n","        if q.startswith(k):\n","          q = v + q[len(k):]\n","  return steps_before_return(q)\n","\n","def steps_before_return(q):\n","  # remove adjancent duplicates\n","  q = remove_duplicates(q)\n","  q = q[0].lower()+q[1:]\n","  return q.strip()\n","\n","def deal_with_no_pronouns_cases(qb_id, q):\n","  qb_id = str(qb_id)\n","  # input: questions after the parse tree steps and before transformation\n","  q = q[0].lower()+q[1:]\n","\n","  question_test = nlp(q)\n","  pronouns_tags = {\"PRON\", \"WDT\", \"WP\", \"WP$\", \"WRB\", \"VEZ\"}\n","  # check whether there are any pronouns or not in the sentence q\n","  flag = True\n","  for token in question_test:\n","    if token.tag_ in pronouns_tags:\n","      flag = False\n","      break\n","  \n","  if flag == True:\n","    # no pronouns in the question\n","\n","    # check wether answer type is singular or plural\n","    answer_type_dict = answer_type_dict_before_parse_tree_nq_like_test_v_3\n","    answer_type = answer_type_dict[qb_id]\n","    processed_text = nlp(answer_type)\n","    lemma_tags = {\"NNS\", \"NNPS\"}\n","\n","    sigular_plural_flags = True # singular\n","    for token in processed_text:\n","      if token.tag_ == 'NNPS':\n","        sigular_plural_flags = False # plural\n","        break\n","    \n","    # check if the first toke is VERB\n","    if question_test[0].pos_ == 'VERB' and question_test[1].pos_ != 'PART' and question_test[2].pos_ != 'AUX':\n","      replacement = 'which '+answer_type+' '\n","      q = replacement+q\n","    else:\n","      if sigular_plural_flags == False:\n","        # plural\n","        replacement = 'which '+answer_type+' are '\n","        q = replacement+q  \n","      else:\n","        # singular\n","        replacement = 'which '+answer_type+' is '\n","        q = replacement+q\n","  # capitalize the first letter of each sentence\n","  q = q[0].upper()+q[1:]\n","  return\n","\n","# transformation from one QB question to a list of NQlike\n","def qb_nq_transformation(qb_id, q):\n","  # parse tree\n","  qb_id = str(qb_id)\n","  nq_like_questions = []\n","\n","  sample = q.strip()\n","  sample = sample.strip('.')\n","  doc = nlp(sample)\n","  seen = set() # keep track of covered words\n","  # Find coref clusters\n","  clusters = doc._.coref_clusters\n","  # Breakdown sentences using Parse Trees\n","  chunks = []\n","  for sent in doc.sents:\n","      conj_heads = [cc for cc in sent.root.children if cc.dep_ == 'conj']\n","      advcl_heads = [cc for cc in sent.root.children if cc.dep_ == 'advcl']\n","      #print('Conjuction Heads found :', conj_heads)\n","      #print('Advcl Heads found :', advcl_heads)\n","    \n","      heads = conj_heads + advcl_heads\n","      for head in heads:\n","          words = [ww for ww in head.subtree]\n","          for word in words:\n","              seen.add(word)\n","\n","          chunk = (' '.join([ww.text for ww in words]))\n","          chunks.append( (head.i, chunk) )\n","\n","      unseen = [ww for ww in sent if ww not in seen]\n","      chunk = ' '.join([ww.text for ww in unseen])\n","      chunks.append( (sent.root.i, chunk) )\n","  \n","  # Sort the chunks based on word index to ensure first sentences formed come first\n","  chunks = sorted(chunks, key=lambda x: x[0])\n","  \n","  # Ensure no sentences aren't too small\n","  if len(chunks)>1:\n","    for idx in range(1, len(chunks)):\n","      try:\n","        curr_i, curr_chunk = chunks[idx]\n","      except:\n","        #print('idx=',idx)\n","        #print('chunk len = ', len(chunks))\n","        raise NotImplementedError\n","      if len(curr_chunk.split()) < 8 or (curr_chunk.split()[0] in ['after']):\n","        #print('\\nFound a small sent!\\n')\n","        last_i, last_chunk = chunks[idx-1]\n","        last_chunk = last_chunk + ' ' + curr_chunk\n","        chunks[idx-1] = (last_i, last_chunk)\n","        del chunks[idx]\n","      if (idx+1)>=len(chunks):\n","        break\n","    curr_i, curr_chunk = chunks[0]\n","    if len(curr_chunk.split()) < 8 and len(chunks)>1:\n","      #print('\\nFound a small pre-sent!\\n')\n","      last_i, next_chunk = chunks[1]\n","      curr_chunk = curr_chunk + ' ' + next_chunk\n","      chunks[0] = (last_i, curr_chunk)\n","      del chunks[1]    \n","  \n","  # Clean each sentence of trailing and, comma etc\n","  for i in range(len(chunks)):\n","    id,chunk = chunks[i]\n","    chunk = clean_chunk(chunk)\n","    chunks[i] = (id, chunk)\n","    \n","  \n","  # Coreference subsitution\n","  pronoun_list = ['he', 'she', 'his', 'her', 'its']\n","  if len(chunks)>1:\n","    for i in range(1, len(chunks)):\n","      curr_i, curr_chunk = chunks[i]\n","      chunk_doc = nlp(curr_chunk)\n","      for id, w in enumerate(chunk_doc[:3]):\n","        #print('Word in chunk doc ', w, '->',w.tag_)\n","        if w.tag_ in ['NN', 'NNP', 'NNS', 'NNPS']:\n","          continue\n","        rep = w.text\n","        for cluster in clusters:\n","          #print('Noun chunks: ', cluster[0], '->', [x for x in cluster[0].noun_chunks])\n","          if (len([x for x in cluster[0].noun_chunks]) > 0) and (str(cluster[0]).lower() not in pronoun_list):\n","            match_cluster = [str(cc) for cc in cluster]\n","            #print(match_cluster)\n","            if w.text in match_cluster:\n","              rep = match_cluster[0]\n","              if w.text.lower() in ['his', 'her', 'its', 'it\\'s']:\n","                rep += '\\'s'\n","              #print(f'Found {w} in cluster!!!')\n","              #print('Replaceing with ', match_cluster[0])\n","              break\n","        if not w.text == rep:\n","          replacement_list = [str(c) for c in chunk_doc] \n","          replacement_list[id] = rep\n","          curr_chunk = (' ').join(replacement_list)\n","          chunks[i] = (curr_i, curr_chunk)\n","        else:\n","          curr_chunk = '' + curr_chunk\n","\n","\n","  #print('\\033[1m'+'Different nq like statements: (after 2nd breakdown):')\n","  for ii, chunk in chunks:\n","    # with the same qid\n","    nq_like_questions.append(chunk)\n","  for i in range(len(nq_like_questions)):\n","    # check if no pronouns in the question\n","    deal_with_no_pronouns_cases(qb_id, nq_like_questions[i])\n","    if i == len(nq_like_questions)-1:\n","      # last sent transformation\n","      nq_like_questions[i] = last_sent_transform(nq_like_questions[i])\n","      quality_check(qb_id, nq_like_questions[i])\n","    else:\n","      # intermediate sent transformation\n","      nq_like_questions[i] = transformation_intermediate_sent(qb_id, nq_like_questions[i])\n","      quality_check(qb_id, nq_like_questions[i])\n","  # return a NQlike list from one qb question\n","  return nq_like_questions"],"metadata":{"id":"58-MqFmgO8t4","colab":{"base_uri":"https://localhost:8080/","height":281,"referenced_widgets":["9098ae6e0a204953b11617eedf7b1219","2b0c6ff4ae784fc4b82492ac578c5351","1bca9e056ab846c2a8b7566bbe8841b3","34c79a1da8364f2d97c67aa47c449184","ec1275eaba4f43c39bf45733fa4be128","aee24222a58b4003b7fefa8a3a60de02","d759285535fc4b88baaac0639dbef427","fc52b623abee4a26a3de28d8dd9b20a2","2659925141304abea67ca31f354b2af9","62300d403bfc4a70893087094287accd","6c82b5a5e4be4568b5e27df2781e4273","1ff39d5d6b4b41dca1fe0e991f57ccaf","d99796f19a5b493683ff1ed4c88f0f8e","b33214e7faf54959bd5ec9e8af6bca1b","e2ec05f450a546ecb9770ed9820589ae","9d610d25ab5949d8a0074e46caaac512","f04ce59518384913b297844c91e7d888","eede79a6209344d494a9e427777bfa13","508ec588b2694899a83e849bb672a464","3b0035e12d004991a83a1f5a150ee041","16e2a23cc5b44b04aef18a39fac41555","0c15793f068a47f4b363af81c489f9f4","dc182d43133d46e982a3004e5ad9569f","ef64fe161997400dbed0f0e947a874ea","ccdad00bcf7d4072b417ed7c9671b48b","7e4c0dde82ac4f9da88daad814679c33","5f982f8698a747f593e2d04a96243639","cbe7a081337a46cea858825f45b49fbd","7c1b3ab747ca4b3b896584f2eb13e25d","6ce481d2f7514bc19e80cb6efff3c526","33a11c3d35d84eada55077dab5d461df","f1298dfcc808487e98d98222ce602139"]},"executionInfo":{"status":"ok","timestamp":1647233917409,"user_tz":240,"elapsed":13738,"user":{"displayName":"Saptarashmi Bandyopadhyay","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16595046178311880344"}},"outputId":"f6364222-b33a-49f5-908e-53d4cf5a5db6"},"execution_count":6,"outputs":[{"output_type":"display_data","data":{"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=28.0, style=ProgressStyle(description_w…"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9098ae6e0a204953b11617eedf7b1219"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["\n"]},{"output_type":"display_data","data":{"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=231508.0, style=ProgressStyle(descripti…"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2659925141304abea67ca31f354b2af9"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["\n"]},{"output_type":"display_data","data":{"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=466062.0, style=ProgressStyle(descripti…"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f04ce59518384913b297844c91e7d888"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["\n"]},{"output_type":"display_data","data":{"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=483.0, style=ProgressStyle(description_…"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ccdad00bcf7d4072b417ed7c9671b48b"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["\n"]},{"output_type":"stream","name":"stderr","text":["All model checkpoint layers were used when initializing TFDistilBertForSequenceClassification.\n","\n","All the layers of TFDistilBertForSequenceClassification were initialized from the model checkpoint at ./BERT_Classification/Aug19_answer_type_classification_model/.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertForSequenceClassification for predictions without further training.\n"]}]},{"cell_type":"code","source":["#Step9. Main\n","\n","if __name__ == \"__main__\":\n","  parser = argparse.ArgumentParser(description=\"Apply heuristic functions\")\n","  parser.add_argument('--limit', type=int, default=20,help=\"Limit of number of QB questions input\")\n","  parser.add_argument('--qb_path', type=str, default='qb_train_with_contexts_lower_nopunc_debug_Feb24.json',\n","                      help=\"path of the qb dataset\")\n","  parser.add_argument('--save_result', type=bool, default=True, help=\"Save NQlike questions with corresponding contexts\")\n","  parser.add_argument('--save_only_NQlike_questions', type=bool, default=False, help=\"Only Save the NQlike outputs\")\n","  parser.add_argument('--answer_type_classifier', type=bool, default=False, help=\"Retrain the answer type classifier from scratch\")\n","  args = parser.parse_args()\n","  # Load dataset\n","  qb_path = args.qb_path\n","  limit = args.limit\n","  qb_df = None\n","  if limit > 0:\n","    qb_df = pd.read_json(qb_path, lines=True, orient='records',nrows=limit)\n","  else:\n","    qb_df = pd.read_json(qb_path, lines=True, orient='records')\n","\n","  qb_questions_input = qb_df['question'].values\n","  qb_id_input = qb_df['qanta_id'].values\n","  # transformation\n","  nq_like_questions_transformation_results = []\n","  for i in range(len(qb_questions_input)):\n","    # transform single QB\n","    q = qb_questions_input[i]\n","    qb_id = qb_id_input[i]\n","    nq_like_questions_lst = qb_nq_transformation(qb_id, q)\n","    nq_like_questions_transformation_results.append(nq_like_questions_lst)\n","  \n","  # save to json as a dataframe\n","  if args.save_result:\n","    nq_like_df = {\n","      'qanta_id':[],\n","      'question':[],\n","      'answer':[],\n","      'char_spans':[],\n","      'context':[]\n","    }\n","    for i in range(len(nq_like_questions_transformation_results)):\n","      assert len(nq_like_questions_transformation_results)==len(qb_df)\n","      nqlist = nq_like_questions_transformation_results[i]\n","      for j in range(len(nqlist)):\n","        nq_like_df['qanta_id'].append(qb_df.iloc[i]['qanta_id'])\n","        nq_like_df['question'].append(nqlist[j])\n","        nq_like_df['answer'].append(qb_df.iloc[i]['answer'])\n","        nq_like_df['char_spans'].append(qb_df.iloc[i]['char_spans'])\n","        nq_like_df['context'].append(qb_df.iloc[i]['context'])\n","    new_nqlike = pd.DataFrame(nq_like_df)\n","    new_nqlike.to_json('./nq_like_questions_train_with_contexts.json', lines=True, orient='records')\n","  # only save the NQLike questions\n","  if args.save_only_NQlike_questions:\n","    with open('./nqlike_questions_outputs.txt', 'w') as f:\n","      for nqlike_lists in nq_like_questions_transformation_results:\n","        for nqlike in nqlike_lists:\n","          f.write(nqlike + \"\\n\")\n","\n","  # retraining the answer type classifier\n","  if args.answer_type_classifier:\n","      print(\"retraining the answer typr classifier from scratch......\")\n","      answer_type_classifier_training()"],"metadata":{"id":"_qrNTteKR_si","executionInfo":{"status":"ok","timestamp":1647234058172,"user_tz":240,"elapsed":25446,"user":{"displayName":"Saptarashmi Bandyopadhyay","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"16595046178311880344"}}},"execution_count":8,"outputs":[]}]}